{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Giới thiệu về thư viện NLTK\n",
        "\n"
      ],
      "metadata": {
        "id": "5nd-4yOTFmJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lgUfRSQEOoC",
        "outputId": "bf504389-941b-486b-8737-3d628ce76957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download_shell()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4piTlxUEeHA",
        "outputId": "0fe54d4a-0f3c-42fc-bd5f-adc4a4f4164e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: \n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "Hit Enter to continue: \n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: \n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "Hit Enter to continue: \n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "Hit Enter to continue: \n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: \n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "Hit Enter to continue: \n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: \n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "Hit Enter to continue: \n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "Hit Enter to continue: \n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: \n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "Hit Enter to continue: \n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: \n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "Hit Enter to continue: \n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "Hit Enter to continue: \n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: \n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "Hit Enter to continue: \n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: \n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "Hit Enter to continue: \n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "Hit Enter to continue: \n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> gutenberg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package gutenberg to /root/nltk_data...\n",
            "      Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns1N9Yf0EZcC",
        "outputId": "3bf00e15-3338-4ed6-e5db-aa80c2b3cb29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print(\"Gutenberg files : \", gb.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gjpNKexFEL4",
        "outputId": "c0f15d64-0d3a-47a4-c5a9-27b697a0feaa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ],
      "metadata": {
        "id": "oWP7_YgzFHCP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(macbeth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10OxJOgnFKDR",
        "outputId": "d8ef077b-e0d1-46aa-d1df-f564ed810827"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth [:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq2gosYfFMH3",
        "outputId": "736b94d9-269d-4e9d-8abf-c992ebf64215"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82bA9uKHFO55",
        "outputId": "18ea04a2-6ef8-47c2-9a39-f8cb4bfb50bf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[',\n",
              "  'The',\n",
              "  'Tragedie',\n",
              "  'of',\n",
              "  'Macbeth',\n",
              "  'by',\n",
              "  'William',\n",
              "  'Shakespeare',\n",
              "  '1603',\n",
              "  ']'],\n",
              " ['Actus', 'Primus', '.'],\n",
              " ['Scoena', 'Prima', '.'],\n",
              " ['Thunder', 'and', 'Lightning', '.'],\n",
              " ['Enter', 'three', 'Witches', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tìm 1 từ với NLTK:"
      ],
      "metadata": {
        "id": "RHyi8DpvFpca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvMQs8gbFtVe",
        "outputId": "b7a13e60-e515-40ac-c139-b5a492df8679"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.common_contexts(['Stage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGR-rLvsFzsl",
        "outputId": "08aefe7d-b02b-4edf-bbf1-b3b3939971e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the_. bloody_: the_,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.similar('Stage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niQbWORqF1Jm",
        "outputId": "2bac7240-0f80-45b1-c249-83e0d6620c45"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day time face warre ayre king bleeding man reuolt serieant like\n",
            "knowledge broyle shew head spring heeles hare thane skie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Phân tích tần số của các từ"
      ],
      "metadata": {
        "id": "daaIDR40F_b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aZFawWwGB0O",
        "outputId": "8cedcc9f-b590-4bbb-e4ec-2c18e7e51841"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzoy6PmkGHGm",
        "outputId": "80ac7a68-05b1-4db3-f5e1-3f8b60b0fc80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4A-1Y0QGJoV",
        "outputId": "db239fcb-ccf1-45fd-e17b-e1eccf2eda9a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from', 'the', 'those', 'him', \"aren't\", 'that', 'll', 'isn', 'have', 'more']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "len(macbeth_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp0FTlSIGLqW",
        "outputId": "2e6bd267-5e1d-4a59-d3d4-aab5a4b67e60"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14946"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtXP5xQJGNU9",
        "outputId": "9248cd6b-bd16-4878-da37-2ca0c3e25bdf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "sw = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "print(fd.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApB8q5L9GPUV",
        "outputId": "f2f0000f-7ee3-4801-8002-200677807e1a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('macb', 137), ('haue', 122), ('thou', 90), ('enter', 81), ('shall', 68), ('macbeth', 62), ('vpon', 62), ('thee', 61), ('macd', 58), ('vs', 57)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Lựa chọn các từ trong văn bản"
      ],
      "metadata": {
        "id": "umYog35gHdRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_words = [w for w in macbeth if len(w)> 12]\n",
        "sorted(long_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oMH1dxZHgc4",
        "outputId": "4910adb4-5167-4c99-a060-d4a038460077"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "sorted(ious_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx9fZRrYHlV_",
        "outputId": "de51f42c-0dcf-4503-8bba-e408e28882ec"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Bigrams và collocations"
      ],
      "metadata": {
        "id": "4zLB_UH_Hnzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3CzH_xzHoa_",
        "outputId": "e0424785-9693-4595-b181-41a7dc988763"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
        "tgrms.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSdPg9dqHpnX",
        "outputId": "62d695be-aa68-470f-bfa5-6bb4451ebac5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Sử dụng văn bản trên mạng"
      ],
      "metadata": {
        "id": "eLiD5zsKHtWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e_xbeHIOHp7W",
        "outputId": "12d6cbbe-f681-4cf0-80db-a67b17964630"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "print(raw[:75])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAqRoPT5HqKu",
        "outputId": "353ac3f3-94c6-4601-ffa4-ba20a1ac5957"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize (raw)\n",
        "webtext = nltk.Text (tokens)\n",
        "webtext[:12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-H_ooxNIHD2",
        "outputId": "2578a80b-34f9-431c-ed27-315d634b9e89"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Project',\n",
              " 'Gutenberg',\n",
              " 'eBook',\n",
              " 'of',\n",
              " 'Crime',\n",
              " 'and',\n",
              " 'Punishment',\n",
              " ',',\n",
              " 'by',\n",
              " 'Fyodor',\n",
              " 'Dostoevsky']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Rút trích văn bản từ trang html"
      ],
      "metadata": {
        "id": "BaN9XqV5IKOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:120]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "S8tcHWCKILj2",
        "outputId": "8a3cbf07-6a5b-4fd2-9d95-c73f36d21e2a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)"
      ],
      "metadata": {
        "id": "2IApm0xmINJW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Phân tích cảm xúc người dùng"
      ],
      "metadata": {
        "id": "Ok1QErqHIOfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSCQEAHtIRFn",
        "outputId": "da6b9fe8-d474-410f-b452-92016083639b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category)\n",
        "for category in reviews.categories()\n",
        "for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "11j0VrLwITdO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_review = ' '.join(documents[0][0])"
      ],
      "metadata": {
        "id": "kP_yt5bwIVRW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxdtgTePIXGG",
        "outputId": "7d96f076-9144-4314-b0c0-4ee783f2186f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now , lets first look into the history of shark films . there was the unforgettable jaws . the exciting jaws 2 . the rather flaky jaws 3d and sometime in the late 90s another film of the same genre that i can ' t seem to recall ( about the son of jaws returning to wreak revenge or something like that ) . now , with the magic of cgi , one shark is simply not enough ; in deep blue sea , there are 3 big , mean and really smart ones ! russell frankiln ( jackson ) visits aquatica ( a sea - bound research center ) , where a research is being conducted on the extraction of a hormone substance found uniquely within the shark brain that can cure and reverse the effects of alzheimer ' s disease . the substance is small in quantity and because of this , lead researcher dr . susan macalaester genetically alters the shark dna and grows them twice the size with brains as big as humans , naturally producing more of the much treasured hormones . as sure as the sun sets in the west , a shark breaks lose and wreaks havoc within the facility during a hormone extraction procedure . with the flexibility offered by cgi , the sharks get more full - length screen time as their predecessors did in those remote - controlled rubber suit days . gone are the days of people getting pulled under the water with the water turning red right after . this time , we get to see the entire gobbling action , with floating limbs and all . so - so acting , expected in most of your average action film . jackson ( fresh from his jedi master role in the phantom menace ) , takes on the dark - side force of a different kind and fits well with his wise - cracking lines . ll cool j ' s cook role as preacher , does for this film what steven seagal ' s cook could only dream to achieve in his two under siege films . deep blue sea does not offer any of the psychological thrills that jaws has to offer . it does however , prove to be a refreshing follow - up within the genre , full of visual thrills , suspense and believe it or not \u0005 humour ! its like jurassic park under water . nothing too stressful , pure entertainment . renny harlin , you are forgiven ( for making cutthroat island ) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PUVheiCnIYOu",
        "outputId": "e33ba5eb-1615-40a5-c5f2-78095755ae43"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)"
      ],
      "metadata": {
        "id": "YiXMIlU-Iajf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(document, word_features):\n",
        "  document_words = set(document)\n",
        "  features = {}\n",
        "  for word in word_features:\n",
        "    features['{}'.format(word)] = (word in document_words)\n",
        "  return features"
      ],
      "metadata": {
        "id": "2dv_1HfIIcOm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
        "len(featuresets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNP7Y58RIiBG",
        "outputId": "7177eea7-c5aa-459a-9dac-5179cd211bda"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "O_aVxf2IInf3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcaa45EUIqCf",
        "outputId": "7727c77f-97e6-40bf-8f7e-18e6aa7f8912"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc9nv8irIrhQ",
        "outputId": "b2e28f52-510a-4412-9c54-61355af2b830"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                   badly = True              neg : pos    =     13.5 : 1.0\n",
            "             outstanding = True              pos : neg    =     11.9 : 1.0\n",
            "               pointless = True              neg : pos    =     10.8 : 1.0\n",
            "               portrayed = True              pos : neg    =     10.6 : 1.0\n",
            "                terrific = True              pos : neg    =      9.4 : 1.0\n",
            "                   awful = True              neg : pos    =      9.1 : 1.0\n",
            "                  bother = True              neg : pos    =      8.7 : 1.0\n",
            "                   stock = True              neg : pos    =      8.7 : 1.0\n",
            "              delightful = True              pos : neg    =      8.6 : 1.0\n",
            "               happiness = True              pos : neg    =      8.0 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BÀI TẬP ÁP DỤNG**"
      ],
      "metadata": {
        "id": "5nh53xKbJaEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Viết chương trình Python với thư viện NLTK để liệt kê các tên của copus."
      ],
      "metadata": {
        "id": "yF4uoatdJlRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvndw_zGJxS_",
        "outputId": "4367d398-343c-46d1-b947-afc9c73c615d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from urllib import request\n",
        "\n",
        "# Tải văn bản từ Project Gutenberg\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "\n",
        "# Tiền xử lý văn bản\n",
        "tokens = word_tokenize(raw)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Sử dụng NLTK để nhận diện thực thể có tên\n",
        "entities = ne_chunk(tagged)\n",
        "\n",
        "# Liệt kê các tên (PERSON)\n",
        "names = []\n",
        "for chunk in entities:\n",
        "    if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "        names.append(' '.join(c[0] for c in chunk))\n",
        "\n",
        "# Loại bỏ các tên trùng lặp\n",
        "unique_names = list(set(names))\n",
        "\n",
        "# In danh sách các tên\n",
        "print(unique_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II4XE_88JqNm",
        "outputId": "948f315d-052e-4428-de96-ca36fc640de6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Really', 'Brother', 'Rodion Ro', 'Zimmerman', 'Noah', 'Hotel', 'Take', 'Mr. Svidrigaïlov', 'Petrovitch', 'Trifles', 'Travels', 'Halfway', 'Within', 'Darkness', 'Lazarus', 'Anxiety', 'Strength', 'Funny', 'Ivan Afanasyvitch', 'Fyodor Dostoevsky Translated', 'Polenka', 'Won', 'Madame Kobilatnikov', 'Water', 'Tchebarov', 'Mamma', 'Fair', 'Love', 'Mother Russia', 'Vahrushin', 'Thy', 'Afanasy Pavlovitch', 'Mihail', 'Scraps', 'Avdotya Romanovna', '_I', 'Please', 'Men', 'Lord', '_Cinq', 'Pfoo', 'Malaya Vishera', 'Rodion Raskolnikov', 'Isn', 'Stop', 'English Character', 'Try', 'Alyona', 'Tobacco', 'Mr. Lebeziatnikov', 'Andrey Semyonovitch', 'Wasn', 'Suffer', 'Dostoevsky', 'Perspiration', 'Darya Frantsovna', 'Make', 'Gentle Sonia', 'Proudhon', 'Boulevard', '_Madonna_', '_Vater_', 'Pulcheria Alexandrovna', 'Instinct', 'Sadovy Street', 'Vassily Ivanovitch Vahrushin', 'Vasya', 'Aie', 'Luise', 'Herewith', 'Delirious', 'Zarnitsyn', 'Porfiry Petrovitch', 'Listen', 'Dmitri Prokofitch Razumihin', 'Amalia Ivanovna', 'Fancy', 'Ivanitch', 'Ivan Mihailovitch', 'Heruvimov', 'Loose', 'Linen', 'Garnett Release Date', 'Email', 'Lewes', 'Poor', 'Speak', 'Will', 'Certain', 'Murmurs', 'Wanderers', 'Dimly', 'Amalia Lippevechsel', 'Mr. Tchebarov', 'Madame Resslich', 'Kozel', 'Prince Svirbey', 'God Himself', 'Diamanten', 'Hurrah', 'Nikolay Dementyev', 'Jove', 'Silence', 'Mr.', 'Mr. Marmeladov', 'Potchinkov', 'Sofya Ivanovna', 'Hitherto', 'Raskolnikov', 'Dounia', 'V', 'Run', 'Bigotry', 'Mary', 'Till', 'Luise Ivanovna', 'Mother', '_He_', 'Pokorev', 'Papa', 'Love Dounia', 'Nastasya Nikiforovna', 'Boots', 'Jews', 'Again Sonia', 'Luckily', 'Avdotya', 'Again', 'Dark', 'Which', '_It', 'Berg', 'Quick', 'Kindly', 'Whom', 'Pray', 'Alyona Ivanovna', 'V Raskolnikov', 'Drunken', 'Wouldn', 'Peasants', 'Ivan', 'Life', 'Sevastopol', 'Mr. Luzhin', 'Widow', 'Reflect', 'Assume', 'Society', 'Mars', 'Rodion Romanovitch', 'Petrovsky Island', 'Project', 'Vassilyevsky Ostrov', 'Arrest', 'Sharmer', 'Ivan Ivanitch Klopstock', 'Poor Lizaveta', 'Pyotr Petrovitch', 'Cruel', 'Project Gutenberg', 'Semyon Semyonovitch', 'Nekrassov', 'Grigoryev', 'Shelopaev', 'Svidrigaïlov', 'Enough', 'Don', 'Sonia Marmeladov', 'Marfa Petrovna', 'Dmitri Prokofitch', 'Petrovsky Park', 'Plain Vanilla', 'Madame de Kapernaumov', 'Vague', 'Amalia Ludwigovna', 'Mr. Raskolnikov', 'Good God', 'Tolstyakov', 'Sofya Semyonova', 'Matvey', 'Man', 'Had Avdotya Romanovna', 'Katia', 'Marry', 'Jesus', 'Bakaleyev', 'Lessons', 'Kolya', 'Juster', 'Mr. Razsudkin', 'Know', 'Donations', 'Katerina', 'Lizaveta', 'Harkov', 'Pestryakov', 'Ideas', 'Clear', 'Appeal', 'Solon', 'Salt Lake City', 'Light', 'Quite', 'Money', 'Write', 'Nil Pavlitch', 'Piderit', 'Alma', 'Providence', 'Terror', 'Live', 'Madonna', 'Andrey Semyonovitch Lebeziatnikov', 'Phew', 'Gogol', 'Avdotya Romanovna Raskolnikov', 'Lie', 'Shouldn', 'Ave Maria', 'Petrovna', 'Rodya', 'Psychologically', 'Poor Marfa Petrovna', 'Philip', 'Sofya', 'Raphael', 'Bend', 'Arkady Ivanovitch', 'Kapernaumov', 'Ludwigovna', 'Thou', 'Achilles', 'Majesty', 'Frenchman', 'Logic', 'Ich', 'Zossimov', 'Amalia', 'Madam', 'Suspicious', 'Tears', 'Pyotr', 'Praskovya Pavlovna', 'Look', 'Marfa', 'Harlamov', 'Honoured', 'Marmeladov', 'Mr. Captain', 'Madame Lippevechsel', 'Rodya Pyotr Petrovitch', 'Mikolka', 'Father', 'Rodion', 'Lizaveta Ivanovna', 'Parasha', 'Pushkin', 'Known', 'Carefully', 'Nobody', 'Pyotr Petrovitch Luzhin', 'Mr. Zametov', 'Marfa Petrovna Svidrigaïlov', 'Nikodim Fomitch', 'Plestcheiev', 'Terebyeva', 'Mercy', 'Mont Blanc', 'Him', 'Anyway', 'Reaumur', 'Koch', 'Perlen_', 'Dmitri', 'Fourier', 'Luzhin', 'Rodion Romanovitch Raskolnikov', 'Mayn', 'Kepler', 'Ark', 'Foo', 'Aren', 'Anyhow', 'Lady', 'Freedom', 'Madeira', 'Dirt', 'Toulon', 'Johann', 'Arkady Ivanovitch Svidrigaïlov', 'Petrovsky', 'How', 'Pass', 'Tit Vassilitch', 'Queer', 'Mr. Razumihin', 'Karl', 'Shan', '_Les Confessions_', 'Wagner', 'Believest', 'Semyon Zaharovitch', 'Fear', 'Couldn', 'Very', 'Sofya Semyonovna', 'Again Dounia', 'Ivanovna', 'Hence', 'Kryukov', 'Stay', 'Imagine', 'Hush', 'Bound', 'Fedosya', 'God', 'Ilya Petrovitch', 'Porfiry', 'Dushkin', 'Babushkin', 'Petersburg', 'Zametov', 'Reason', 'Nikolay', 'Project Gutenberg-tm', 'Martha', 'Nicholas', 'Bedlam', 'Veal', 'Afanasy Ivanovitch Vahrushin', 'Petrovsky Ostrov', 'Alexandr Grigorievitch', 'Michael S. Hart', 'Towards', 'Easter', 'Good', '_I_', 'Pashenka', 'Nastasya', 'Deep', 'Fearful', 'Lida', 'Stand', 'Sonia', 'Dourov', 'Mitka', 'Done', 'Almost', 'Afanasy Ivanovitch', 'Guard', 'Someone', 'Alexey Semyonovitch', 'Catch', '_Crevez', 'Henriette', 'Blood', 'Resslich', 'Katerina Ivanovna', 'Better', 'Dagny', 'Razumihin', 'Simply', 'Mahomet', 'Who', 'Such', 'Amalia Fyodorovna', 'Berlin_', 'Aniska', 'Bitter', 'Alexandrovna', 'John Bickers', 'Fyodor Dostoevsky', 'Sofya Semyonovna Marmeladov', 'Thank God', 'Sit', 'Prince Schegolskoy', 'Schiller', 'Mistrustfully', 'Rather', 'Wait', 'Home', 'Ach', 'Napoleon', 'Hadn', 'Varents', 'Oughtn', 'Soon', 'V Lebeziatnikov', 'Better Porfiry', 'Strange', 'Fool', 'Vassilyevsky Prospect', 'Pain', 'Didn', 'Haven', 'David Widger', 'Lebeziatnikov', 'Rousseau', 'Third Street', 'Knopp', 'Privy Councillors', 'Krestovsky Island', 'Natalya Yegorovna', 'Buch', 'Aha', 'Bravo', 'Idiot', 'Vrazumihin', 'Truth', 'Where', 'Yours', 'Farewell', 'Shivers', 'Explain', 'Turn', 'Barmherzige_']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Viết chương trình Python với thư viện NLTK để liệt kê danh sách các stopword bằng các ngôn ngữ khác nhau."
      ],
      "metadata": {
        "id": "d9mFR35KKMma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Tải các tài nguyên cần thiết\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Lấy danh sách các ngôn ngữ có sẵn\n",
        "languages = stopwords.fileids()\n",
        "\n",
        "# Tạo từ điển chứa danh sách stopwords cho từng ngôn ngữ\n",
        "stopwords_dict = {lang: stopwords.words(lang) for lang in languages}\n",
        "\n",
        "# In danh sách các ngôn ngữ và số lượng stopwords\n",
        "for lang, words in stopwords_dict.items():\n",
        "    print(f\"Language: {lang}, Number of stopwords: {len(words)}\")\n",
        "    print(f\"Sample stopwords: {words[:10]}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Lưu danh sách stopwords vào file\n",
        "with open('stopwords_list.txt', 'w', encoding='utf-8') as f:\n",
        "    for lang, words in stopwords_dict.items():\n",
        "        f.write(f\"Language: {lang}\\n\")\n",
        "        f.write(f\"Stopwords: {', '.join(words)}\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ts1MfrKVlQ",
        "outputId": "249aaeda-13b9-40cd-f8da-f1076756fb77"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: arabic, Number of stopwords: 754\n",
            "Sample stopwords: ['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي']\n",
            "----------------------------------------\n",
            "Language: azerbaijani, Number of stopwords: 165\n",
            "Sample stopwords: ['a', 'ad', 'altı', 'altmış', 'amma', 'arasında', 'artıq', 'ay', 'az', 'bax']\n",
            "----------------------------------------\n",
            "Language: basque, Number of stopwords: 326\n",
            "Sample stopwords: ['ahala', 'aitzitik', 'al', 'ala ', 'alabadere', 'alabaina', 'alabaina', 'aldiz ', 'alta', 'amaitu']\n",
            "----------------------------------------\n",
            "Language: bengali, Number of stopwords: 398\n",
            "Sample stopwords: ['অতএব', 'অথচ', 'অথবা', 'অনুযায়ী', 'অনেক', 'অনেকে', 'অনেকেই', 'অন্তত', 'অন্য', 'অবধি']\n",
            "----------------------------------------\n",
            "Language: catalan, Number of stopwords: 278\n",
            "Sample stopwords: ['a', 'abans', 'ací', 'ah', 'així', 'això', 'al', 'aleshores', 'algun', 'alguna']\n",
            "----------------------------------------\n",
            "Language: chinese, Number of stopwords: 841\n",
            "Sample stopwords: ['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时']\n",
            "----------------------------------------\n",
            "Language: danish, Number of stopwords: 94\n",
            "Sample stopwords: ['og', 'i', 'jeg', 'det', 'at', 'en', 'den', 'til', 'er', 'som']\n",
            "----------------------------------------\n",
            "Language: dutch, Number of stopwords: 101\n",
            "Sample stopwords: ['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij']\n",
            "----------------------------------------\n",
            "Language: english, Number of stopwords: 179\n",
            "Sample stopwords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
            "----------------------------------------\n",
            "Language: finnish, Number of stopwords: 235\n",
            "Sample stopwords: ['olla', 'olen', 'olet', 'on', 'olemme', 'olette', 'ovat', 'ole', 'oli', 'olisi']\n",
            "----------------------------------------\n",
            "Language: french, Number of stopwords: 157\n",
            "Sample stopwords: ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']\n",
            "----------------------------------------\n",
            "Language: german, Number of stopwords: 232\n",
            "Sample stopwords: ['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']\n",
            "----------------------------------------\n",
            "Language: greek, Number of stopwords: 265\n",
            "Sample stopwords: ['αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτεσ', 'αυτη', 'αυτο', 'αυτοι', 'αυτοσ']\n",
            "----------------------------------------\n",
            "Language: hebrew, Number of stopwords: 221\n",
            "Sample stopwords: ['אני', 'את', 'אתה', 'אנחנו', 'אתן', 'אתם', 'הם', 'הן', 'היא', 'הוא']\n",
            "----------------------------------------\n",
            "Language: hinglish, Number of stopwords: 1036\n",
            "Sample stopwords: ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab']\n",
            "----------------------------------------\n",
            "Language: hungarian, Number of stopwords: 199\n",
            "Sample stopwords: ['a', 'ahogy', 'ahol', 'aki', 'akik', 'akkor', 'alatt', 'által', 'általában', 'amely']\n",
            "----------------------------------------\n",
            "Language: indonesian, Number of stopwords: 758\n",
            "Sample stopwords: ['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n",
            "----------------------------------------\n",
            "Language: italian, Number of stopwords: 279\n",
            "Sample stopwords: ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']\n",
            "----------------------------------------\n",
            "Language: kazakh, Number of stopwords: 324\n",
            "Sample stopwords: ['ах', 'ох', 'эх', 'ай', 'эй', 'ой', 'тағы', 'тағыда', 'әрине', 'жоқ']\n",
            "----------------------------------------\n",
            "Language: nepali, Number of stopwords: 255\n",
            "Sample stopwords: ['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने']\n",
            "----------------------------------------\n",
            "Language: norwegian, Number of stopwords: 176\n",
            "Sample stopwords: ['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er']\n",
            "----------------------------------------\n",
            "Language: portuguese, Number of stopwords: 207\n",
            "Sample stopwords: ['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n",
            "----------------------------------------\n",
            "Language: romanian, Number of stopwords: 356\n",
            "Sample stopwords: ['a', 'abia', 'acea', 'aceasta', 'această', 'aceea', 'aceeasi', 'acei', 'aceia', 'acel']\n",
            "----------------------------------------\n",
            "Language: russian, Number of stopwords: 151\n",
            "Sample stopwords: ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']\n",
            "----------------------------------------\n",
            "Language: slovene, Number of stopwords: 1784\n",
            "Sample stopwords: ['ali', 'ampak', 'bodisi', 'in', 'kajti', 'marveč', 'namreč', 'ne', 'niti', 'oziroma']\n",
            "----------------------------------------\n",
            "Language: spanish, Number of stopwords: 313\n",
            "Sample stopwords: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n",
            "----------------------------------------\n",
            "Language: swedish, Number of stopwords: 114\n",
            "Sample stopwords: ['och', 'det', 'att', 'i', 'en', 'jag', 'hon', 'som', 'han', 'på']\n",
            "----------------------------------------\n",
            "Language: tajik, Number of stopwords: 163\n",
            "Sample stopwords: ['аз', 'дар', 'ба', 'бо', 'барои', 'бе', 'то', 'ҷуз', 'пеши', 'назди']\n",
            "----------------------------------------\n",
            "Language: turkish, Number of stopwords: 53\n",
            "Sample stopwords: ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz']\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Viết chương trình Python với thư viện NLTK để kiểm tra danh sách các stopword bằng các\n",
        "ngôn ngữ khác nhau."
      ],
      "metadata": {
        "id": "D-itFcjMKYvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Tải các tài nguyên cần thiết\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Lấy danh sách các ngôn ngữ có sẵn\n",
        "languages = stopwords.fileids()\n",
        "\n",
        "# Tạo từ điển chứa danh sách stopwords cho từng ngôn ngữ\n",
        "stopwords_dict = {lang: stopwords.words(lang) for lang in languages}\n",
        "\n",
        "def check_stopwords(language):\n",
        "    if language in stopwords_dict:\n",
        "        print(f\"Stopwords for language '{language}':\")\n",
        "        print(stopwords_dict[language])\n",
        "    else:\n",
        "        print(f\"Language '{language}' is not available in the stopwords list.\")\n",
        "\n",
        "# Kiểm tra danh sách stopwords bằng cách nhập tên ngôn ngữ\n",
        "while True:\n",
        "    lang = input(\"Enter a language to check its stopwords (or 'exit' to quit): \").strip().lower()\n",
        "    if lang == 'exit':\n",
        "        break\n",
        "    check_stopwords(lang)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI-VXuwUKcao",
        "outputId": "b0888826-b0b6-4d28-a9a0-4248fa24173e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a language to check its stopwords (or 'exit' to quit): english\n",
            "Stopwords for language 'english':\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "Enter a language to check its stopwords (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Viết chương trình Python với thư viện NLTK để loại bỏ các stopword từ một văn bản đã cho."
      ],
      "metadata": {
        "id": "2tTyLdnaLUTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Văn bản ví dụ\n",
        "text = \"This is a sample text with some stopwords that need to be removed.\"\n",
        "\n",
        "# Tokenize văn bản\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Lấy danh sách các từ dừng trong tiếng Anh\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Loại bỏ các từ dừng\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Kết quả sau khi loại bỏ các từ dừng\n",
        "filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "print(\"Original text:\", text)\n",
        "print(\"Filtered text:\", filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6L2WRr0LQ23",
        "outputId": "9b9e2c58-433f-4d20-948a-44c10cd36ff2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: This is a sample text with some stopwords that need to be removed.\n",
            "Filtered text: sample text stopwords need removed .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Viết chương trình Python với thư viện NLTK bỏ qua các stopword từ danh sách\n",
        "các\n",
        "stopword"
      ],
      "metadata": {
        "id": "E9L9H7UALizc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Văn bản ví dụ\n",
        "text = \"This is a sample text with some stopwords that need to be removed.\"\n",
        "\n",
        "# Tokenize văn bản\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Lấy danh sách các từ dừng trong tiếng Anh\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Loại bỏ các từ dừng khỏi danh sách các từ dừng (giả sử muốn loại bỏ 'the', 'is' từ stopwords)\n",
        "custom_stop_words = {'the', 'is'}\n",
        "filtered_stop_words = stop_words - custom_stop_words\n",
        "\n",
        "print(\"Original stopwords count:\", len(stop_words))\n",
        "print(\"Filtered stopwords count:\", len(filtered_stop_words))\n",
        "print(\"Filtered stopwords:\", filtered_stop_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r20iqZoqLzfP",
        "outputId": "f48f2cf8-f20b-42f6-c23d-307d59705f5f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original stopwords count: 179\n",
            "Filtered stopwords count: 177\n",
            "Filtered stopwords: {'from', 'those', 'him', \"aren't\", 'that', 'll', 'isn', 'have', 'more', 'shouldn', 'who', 'once', \"doesn't\", 'should', 'over', 'm', 'having', 'each', 'these', 'some', 'such', 'their', 'on', \"shan't\", \"that'll\", \"wasn't\", 'haven', 'nor', 'at', 'above', 'aren', 'out', \"won't\", 'own', 'by', 'not', 'yours', 'no', 'o', 'her', 'through', 'when', 'you', 'under', \"should've\", 'but', 'and', 'i', \"weren't\", 'an', \"mightn't\", 'can', \"shouldn't\", 'a', 'to', 'hers', 'all', 'couldn', 'where', 'hadn', 'than', 'if', 'she', 'them', \"you'd\", 'be', 's', 'me', 'our', 'up', 'during', 'was', 'do', 'both', 'mightn', 'y', 'my', \"hadn't\", 'how', 'into', 'did', 'he', 'with', 'this', \"isn't\", 'yourselves', 'doing', 'does', 'because', \"mustn't\", 'while', 'just', 'herself', 'off', 'needn', 'few', 'whom', 'wouldn', \"it's\", \"wouldn't\", 'between', 'his', 'most', 'ourselves', 'been', 'had', 'myself', 'only', 'd', 'below', 'hasn', 'very', 'doesn', 'after', \"couldn't\", \"haven't\", 'again', 'they', 'being', 'of', 'don', \"you've\", 'were', 'ain', 'until', 'are', 'or', 'mustn', 'now', 'further', 'ours', 'other', 'has', 'so', 'himself', \"needn't\", \"she's\", 'against', 'its', 'which', 'same', 'why', 'won', \"you're\", 'then', 'what', 'for', 'before', 'yourself', \"hasn't\", 'down', 'we', 'itself', 'about', 'shan', \"didn't\", 'ma', 'as', 'themselves', 't', 'didn', 'here', 'any', 'am', 're', 've', 'there', 'theirs', 'in', 'wasn', 'weren', 'your', 'will', 'it', 'too', \"you'll\", \"don't\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Viết một chương trình Python với thư viện NLTK để tìm định nghĩa và ví dụ của một từ đã\n",
        "cho bằng WordNet từ Wikipedia"
      ],
      "metadata": {
        "id": "idf2oCasMegI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Tải tài nguyên cần thiết\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonyms_and_relations(word):\n",
        "    # Tìm các synsets (tập hợp đồng nghĩa) của từ\n",
        "    synsets = wn.synsets(word)\n",
        "\n",
        "    # Nếu không tìm thấy synsets nào, báo lỗi\n",
        "    if not synsets:\n",
        "        print(f\"No synonyms found for the word '{word}'.\")\n",
        "        return\n",
        "\n",
        "    # In các đồng nghĩa và mối quan hệ từ ngữ cho mỗi synset\n",
        "    for synset in synsets:\n",
        "        print(f\"Synonyms (Lemma names): {', '.join([lemma.name() for lemma in synset.lemmas()])}\")\n",
        "        print(f\"Definition: {synset.definition()}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Từ cần tìm đồng nghĩa và mối quan hệ từ ngữ\n",
        "word = input(\"Enter a word to find its synonyms and lexical relations: \").strip().lower()\n",
        "\n",
        "# Tìm đồng nghĩa và mối quan hệ từ ngữ\n",
        "get_synonyms_and_relations(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIDap3X0Mdic",
        "outputId": "308cc266-eb96-4f20-ca6f-2fdb8c904dcb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word to find its synonyms and lexical relations: play\n",
            "Synonyms (Lemma names): play, drama, dramatic_play\n",
            "Definition: a dramatic work intended for performance by actors on a stage\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: a theatrical performance of a drama\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: a preset plan of action in team sports\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): maneuver, manoeuvre, play\n",
            "Definition: a deliberate coordinated movement requiring dexterity and skill\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: a state in which action is feasible\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: utilization or exercise\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): bid, play\n",
            "Definition: an attempt to get something\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play, child's_play\n",
            "Definition: activity by children that is guided more by imagination than by fixed rules\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): playing_period, period_of_play, play\n",
            "Definition: (in games or plays or other performances) the time during which play proceeds\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): free_rein, play\n",
            "Definition: the removal of constraints\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): shimmer, play\n",
            "Definition: a weak and tremulous light\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): fun, play, sport\n",
            "Definition: verbal wit or mockery (often at another's expense but not to be taken seriously)\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): looseness, play\n",
            "Definition: movement or space for movement\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play, frolic, romp, gambol, caper\n",
            "Definition: gay or light-hearted recreational activity for diversion or amusement\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): turn, play\n",
            "Definition: (game) the activity of doing something in an agreed succession\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): gambling, gaming, play\n",
            "Definition: the act of playing for stakes in the hope of winning (including the payment of a price for a chance to win a prize)\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play, swordplay\n",
            "Definition: the act using a sword (or other weapon) vigorously and skillfully\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: participate in games or sport\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: act or have an effect in a specified way or with a specific effect or outcome\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: play on an instrument\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): act, play, represent\n",
            "Definition: play a role or part\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: be at play; be engaged in playful activity; amuse oneself in a way characteristic of children\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play, spiel\n",
            "Definition: replay (as a melody)\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: perform music on (a musical instrument)\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): act, play, act_as\n",
            "Definition: pretend to have certain qualities or state of mind\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: move or seem to move quickly, lightly, or irregularly\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: bet or wager (money)\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play, recreate\n",
            "Definition: engage in recreational activities rather than work; occupy oneself in a diversion\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: pretend to be somebody in the framework of a game or playful activity\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: emit recorded sound\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: perform on a certain location\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: put (a card or piece) into play during a game, or act strategically as if in a card game\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play, toy\n",
            "Definition: engage in an activity as if it were a game rather than take it seriously\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: behave in a certain way\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play, run\n",
            "Definition: cause to emit recorded audio or video\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): toy, fiddle, diddle, play\n",
            "Definition: manipulate manually or in one's mind or imagination\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: use to one's advantage\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): dally, trifle, play\n",
            "Definition: consider not very seriously\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: be received or accepted or interpreted in a specific way\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): dally, toy, play, flirt\n",
            "Definition: behave carelessly or indifferently\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: cause to move or operate freely within a bounded space\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): act, play, roleplay, playact\n",
            "Definition: perform on a stage or theater\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: be performed or presented for public viewing\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): bring, work, play, wreak, make_for\n",
            "Definition: cause to happen or to occur as a consequence\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: discharge or direct or be discharged or directed as if in a continuous stream\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: make bets\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): bet, wager, play\n",
            "Definition: stake on the outcome of an issue\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: shoot or hit in a particular manner\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: use or move\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: employ in a game or in a specific position\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): meet, encounter, play, take_on\n",
            "Definition: contend against an opponent in a sport, game, or battle\n",
            "----------------------------------------\n",
            "Synonyms (Lemma names): play\n",
            "Definition: exhaust by allowing to pull on the line\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Viết chương trình Python với thư viện NLTK để tìm tập hợp các từ đồng nghĩa và trái nghĩa\n",
        "của một từ nào đó."
      ],
      "metadata": {
        "id": "QVmu-CkbMszV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Tải tài nguyên cần thiết\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonyms_antonyms(word):\n",
        "    synonyms = []\n",
        "    antonyms = []\n",
        "\n",
        "    # Tìm các synsets (tập hợp các từ có cùng ý nghĩa) của từ\n",
        "    synsets = wn.synsets(word)\n",
        "\n",
        "    # Duyệt qua từng synset để lấy các đồng nghĩa và trái nghĩa\n",
        "    for synset in synsets:\n",
        "        for lemma in synset.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "            if lemma.antonyms():\n",
        "                antonyms.append(lemma.antonyms()[0].name())\n",
        "\n",
        "    # Loại bỏ các từ trùng lặp\n",
        "    synonyms = list(set(synonyms))\n",
        "    antonyms = list(set(antonyms))\n",
        "\n",
        "    return synonyms, antonyms\n",
        "\n",
        "# Từ cần tìm đồng nghĩa và trái nghĩa\n",
        "word = input(\"Nhập từ cần tìm đồng nghĩa và trái nghĩa: \").strip().lower()\n",
        "\n",
        "# Gọi hàm để lấy đồng nghĩa và trái nghĩa\n",
        "synonyms, antonyms = get_synonyms_antonyms(word)\n",
        "\n",
        "# In kết quả\n",
        "print(f\"Đồng nghĩa của từ '{word}': {', '.join(synonyms)}\")\n",
        "print(f\"Trái nghĩa của từ '{word}': {', '.join(antonyms)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzq1gXYvM2oZ",
        "outputId": "f2aec7b6-b8a0-43f1-d62a-3d184a7c2f2d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nhập từ cần tìm đồng nghĩa và trái nghĩa: good\n",
            "Đồng nghĩa của từ 'good': in_effect, expert, well, just, dependable, estimable, honest, effective, secure, soundly, ripe, in_force, right, skillful, near, good, unspoiled, goodness, serious, undecomposed, sound, unspoilt, proficient, skilful, practiced, upright, trade_good, safe, salutary, commodity, dear, thoroughly, beneficial, honorable, respectable, adept, full\n",
            "Trái nghĩa của từ 'good': bad, ill, badness, evil, evilness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Viết chương trình Python với thư viện NLTK để có cái nhìn tổng quan về bộ tag, chi tiết của\n",
        "một tag cụ thể trong bộ tag và chi tiết về một số bộ tag liên quan, sử dụng biểu thức chính\n",
        "quy."
      ],
      "metadata": {
        "id": "c6Ln48o2M-pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download necessary resources if not already downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def overview_pos_tags():\n",
        "    print(\"Bộ tag chính:\")\n",
        "    print(\"--------------\")\n",
        "    # Print all POS tags available in WordNet\n",
        "    print(wn.all_lemma_names())\n",
        "    print()\n",
        "\n",
        "def detail_tag(tag):\n",
        "    print(f\"Chi tiết của bộ tag '{tag}':\")\n",
        "    print(\"------------------------------\")\n",
        "    # Find synsets (sets of synonymous words) for the given tag\n",
        "    synsets = wn.synsets(tag)\n",
        "    if synsets:\n",
        "        for synset in synsets:\n",
        "            print(f\"Synset: {synset.name()}\")\n",
        "            print(f\"Definition: {synset.definition()}\")\n",
        "            print(f\"Examples: {synset.examples()}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(f\"Bộ tag '{tag}' không tồn tại trong WordNet.\")\n",
        "    print()\n",
        "\n",
        "def related_tag_patterns():\n",
        "    print(\"Một số biểu thức chính quy liên quan:\")\n",
        "    print(\"------------------------------------\")\n",
        "    # Print some related regular expression patterns (if needed)\n",
        "    print(\"Tùy vào từng yêu cầu sử dụng biểu thức chính quy.\")\n",
        "    print()\n",
        "\n",
        "# Run the functions to print information\n",
        "overview_pos_tags()\n",
        "\n",
        "# Detail of a specific tag (you can change 'good' to any other tag you want to explore)\n",
        "detail_tag('good')\n",
        "\n",
        "# Print related tag patterns (optional)\n",
        "related_tag_patterns()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5c7BqKXM_mp",
        "outputId": "d3e01fc7-23e7-42f5-e631-ab3fdc705f63"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bộ tag chính:\n",
            "--------------\n",
            "<dict_keyiterator object at 0x7c6ba1d32e30>\n",
            "\n",
            "Chi tiết của bộ tag 'good':\n",
            "------------------------------\n",
            "Synset: good.n.01\n",
            "Definition: benefit\n",
            "Examples: ['for your own good', \"what's the good of worrying?\"]\n",
            "\n",
            "Synset: good.n.02\n",
            "Definition: moral excellence or admirableness\n",
            "Examples: ['there is much good to be found in people']\n",
            "\n",
            "Synset: good.n.03\n",
            "Definition: that which is pleasing or valuable or useful\n",
            "Examples: ['weigh the good against the bad', 'among the highest goods of all are happiness and self-realization']\n",
            "\n",
            "Synset: commodity.n.01\n",
            "Definition: articles of commerce\n",
            "Examples: []\n",
            "\n",
            "Synset: good.a.01\n",
            "Definition: having desirable or positive qualities especially those suitable for a thing specified\n",
            "Examples: ['good news from the hospital', 'a good report card', 'when she was good she was very very good', 'a good knife is one good for cutting', 'this stump will make a good picnic table', 'a good check', 'a good joke', 'a good exterior paint', 'a good secretary', 'a good dress for the office']\n",
            "\n",
            "Synset: full.s.06\n",
            "Definition: having the normally expected amount\n",
            "Examples: ['gives full measure', 'gives good measure', 'a good mile from here']\n",
            "\n",
            "Synset: good.a.03\n",
            "Definition: morally admirable\n",
            "Examples: []\n",
            "\n",
            "Synset: estimable.s.02\n",
            "Definition: deserving of esteem and respect\n",
            "Examples: ['all respectable companies give guarantees', \"ruined the family's good name\"]\n",
            "\n",
            "Synset: beneficial.s.01\n",
            "Definition: promoting or enhancing well-being\n",
            "Examples: ['an arms limitation agreement beneficial to all countries', 'the beneficial effects of a temperate climate', 'the experience was good for her']\n",
            "\n",
            "Synset: good.s.06\n",
            "Definition: agreeable or pleasing\n",
            "Examples: ['we all had a good time', 'good manners']\n",
            "\n",
            "Synset: good.s.07\n",
            "Definition: of moral excellence\n",
            "Examples: ['a genuinely good person', 'a just cause', 'an upright and respectable man']\n",
            "\n",
            "Synset: adept.s.01\n",
            "Definition: having or showing knowledge and skill and aptitude\n",
            "Examples: ['adept in handicrafts', 'an adept juggler', 'an expert job', 'a good mechanic', 'a practiced marksman', 'a proficient engineer', 'a lesser-known but no less skillful composer', 'the effect was achieved by skillful retouching']\n",
            "\n",
            "Synset: good.s.09\n",
            "Definition: thorough\n",
            "Examples: ['had a good workout', 'gave the house a good cleaning']\n",
            "\n",
            "Synset: dear.s.02\n",
            "Definition: with or in a close or intimate relationship\n",
            "Examples: ['a good friend', 'my sisters and brothers are near and dear']\n",
            "\n",
            "Synset: dependable.s.04\n",
            "Definition: financially sound\n",
            "Examples: ['a good investment', 'a secure investment']\n",
            "\n",
            "Synset: good.s.12\n",
            "Definition: most suitable or right for a particular purpose\n",
            "Examples: ['a good time to plant tomatoes', 'the right time to act', 'the time is ripe for great sociological changes']\n",
            "\n",
            "Synset: good.s.13\n",
            "Definition: resulting favorably\n",
            "Examples: [\"it's a good thing that I wasn't there\", 'it is good that you stayed', 'it is well that no one saw you', \"all's well that ends well\"]\n",
            "\n",
            "Synset: effective.s.04\n",
            "Definition: exerting force or influence\n",
            "Examples: ['the law is effective immediately', 'a warranty good for two years', 'the law is already in effect (or in force)']\n",
            "\n",
            "Synset: good.s.15\n",
            "Definition: capable of pleasing\n",
            "Examples: ['good looks']\n",
            "\n",
            "Synset: good.s.16\n",
            "Definition: appealing to the mind\n",
            "Examples: ['good music', 'a serious book']\n",
            "\n",
            "Synset: good.s.17\n",
            "Definition: in excellent physical condition\n",
            "Examples: ['good teeth', 'I still have one good leg', 'a sound mind in a sound body']\n",
            "\n",
            "Synset: good.s.18\n",
            "Definition: tending to promote physical well-being; beneficial to health\n",
            "Examples: ['beneficial effects of a balanced diet', \"a good night's sleep\", 'the salutary influence of pure air']\n",
            "\n",
            "Synset: good.s.19\n",
            "Definition: not forged\n",
            "Examples: ['a good dollar bill']\n",
            "\n",
            "Synset: good.s.20\n",
            "Definition: not left to spoil\n",
            "Examples: ['the meat is still good']\n",
            "\n",
            "Synset: good.s.21\n",
            "Definition: generally admired\n",
            "Examples: ['good taste']\n",
            "\n",
            "Synset: well.r.01\n",
            "Definition: (often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\n",
            "Examples: ['the children behaved well', 'a task well done', 'the party went well', 'he slept well', 'a well-argued thesis', 'a well-seasoned dish', 'a well-planned party', 'the baby can walk pretty good']\n",
            "\n",
            "Synset: thoroughly.r.02\n",
            "Definition: completely and absolutely (`good' is sometimes used informally for `thoroughly')\n",
            "Examples: ['he was soundly defeated', 'we beat him good']\n",
            "\n",
            "\n",
            "Một số biểu thức chính quy liên quan:\n",
            "------------------------------------\n",
            "Tùy vào từng yêu cầu sử dụng biểu thức chính quy.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai danh từ đã\n",
        "cho."
      ],
      "metadata": {
        "id": "Oe6v_SCvNQJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Tải tài nguyên cần thiết\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def compare_nouns_similarity(noun1, noun2):\n",
        "    # Tìm các synsets (tập hợp các từ có cùng ý nghĩa) của hai danh từ\n",
        "    synsets1 = wn.synsets(noun1, pos=wn.NOUN)\n",
        "    synsets2 = wn.synsets(noun2, pos=wn.NOUN)\n",
        "\n",
        "    # Nếu không tìm thấy synsets cho bất kỳ danh từ nào, thông báo lỗi\n",
        "    if not synsets1:\n",
        "        print(f\"Không tìm thấy synsets cho danh từ '{noun1}'.\")\n",
        "        return\n",
        "    if not synsets2:\n",
        "        print(f\"Không tìm thấy synsets cho danh từ '{noun2}'.\")\n",
        "        return\n",
        "\n",
        "    # Tính độ tương đồng của các synsets\n",
        "    max_similarity = 0\n",
        "    for synset1 in synsets1:\n",
        "        for synset2 in synsets2:\n",
        "            similarity = synset1.path_similarity(synset2)\n",
        "            if similarity is not None and similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_pair = (synset1, synset2)\n",
        "\n",
        "    # In kết quả\n",
        "    if max_similarity > 0:\n",
        "        print(f\"Độ tương đồng lớn nhất giữa '{noun1}' và '{noun2}': {max_similarity}\")\n",
        "        print(f\"Synset của '{noun1}': {best_pair[0]}\")\n",
        "        print(f\"Synset của '{noun2}': {best_pair[1]}\")\n",
        "        print(f\"Định nghĩa của '{noun1}': {best_pair[0].definition()}\")\n",
        "        print(f\"Định nghĩa của '{noun2}': {best_pair[1].definition()}\")\n",
        "    else:\n",
        "        print(f\"Không tìm thấy độ tương đồng nào giữa '{noun1}' và '{noun2}'.\")\n",
        "\n",
        "# Nhập hai danh từ cần so sánh\n",
        "noun1 = input(\"Nhập danh từ thứ nhất: \").strip().lower()\n",
        "noun2 = input(\"Nhập danh từ thứ hai: \").strip().lower()\n",
        "\n",
        "# Gọi hàm để so sánh độ tương đồng\n",
        "compare_nouns_similarity(noun1, noun2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K36qj7XNXTg",
        "outputId": "39096f44-5479-4895-80da-f7227e4aed65"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nhập danh từ thứ nhất: car\n",
            "Nhập danh từ thứ hai: vehicle\n",
            "Độ tương đồng lớn nhất giữa 'car' và 'vehicle': 0.3333333333333333\n",
            "Synset của 'car': Synset('car.n.02')\n",
            "Synset của 'vehicle': Synset('vehicle.n.01')\n",
            "Định nghĩa của 'car': a wheeled vehicle adapted to the rails of railroad\n",
            "Định nghĩa của 'vehicle': a conveyance that transports people or objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai động từ đã\n",
        "cho"
      ],
      "metadata": {
        "id": "1uQp1iv5NdOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def compare_verbs_similarity(verb1, verb2):\n",
        "    # Tìm các synsets (tập hợp các từ có cùng ý nghĩa) của hai động từ\n",
        "    synsets1 = wn.synsets(verb1, pos=wn.VERB)\n",
        "    synsets2 = wn.synsets(verb2, pos=wn.VERB)\n",
        "\n",
        "    # Nếu không tìm thấy synsets cho bất kỳ động từ nào, thông báo lỗi\n",
        "    if not synsets1:\n",
        "        print(f\"Không tìm thấy synsets cho động từ '{verb1}'.\")\n",
        "        return\n",
        "    if not synsets2:\n",
        "        print(f\"Không tìm thấy synsets cho động từ '{verb2}'.\")\n",
        "        return\n",
        "\n",
        "    # Tính độ tương đồng của các synsets\n",
        "    max_similarity = 0\n",
        "    for synset1 in synsets1:\n",
        "        for synset2 in synsets2:\n",
        "            similarity = synset1.path_similarity(synset2)\n",
        "            if similarity is not None and similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_pair = (synset1, synset2)\n",
        "\n",
        "    # In kết quả\n",
        "    if max_similarity > 0:\n",
        "        print(f\"Độ tương đồng lớn nhất giữa '{verb1}' và '{verb2}': {max_similarity}\")\n",
        "        print(f\"Synset của '{verb1}': {best_pair[0]}\")\n",
        "        print(f\"Synset của '{verb2}': {best_pair[1]}\")\n",
        "        print(f\"Định nghĩa của '{verb1}': {best_pair[0].definition()}\")\n",
        "        print(f\"Định nghĩa của '{verb2}': {best_pair[1].definition()}\")\n",
        "    else:\n",
        "        print(f\"Không tìm thấy độ tương đồng nào giữa '{verb1}' và '{verb2}'.\")\n",
        "\n",
        "# Nhập hai động từ cần so sánh\n",
        "verb1 = input(\"Nhập động từ thứ nhất: \").strip().lower()\n",
        "verb2 = input(\"Nhập động từ thứ hai: \").strip().lower()\n",
        "\n",
        "# Gọi hàm để so sánh độ tương đồng\n",
        "compare_verbs_similarity(verb1, verb2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeuYwycmNmmo",
        "outputId": "bba63c80-189b-425c-d538-3c686e4072aa"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nhập động từ thứ nhất: run\n",
            "Nhập động từ thứ hai: walk\n",
            "Độ tương đồng lớn nhất giữa 'run' và 'walk': 0.3333333333333333\n",
            "Synset của 'run': Synset('run.v.11')\n",
            "Synset của 'walk': Synset('walk.v.01')\n",
            "Định nghĩa của 'run': move about freely and without restraint, or act as if running around in an uncontrolled way\n",
            "Định nghĩa của 'walk': use one's feet to advance; advance by steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Viết chương trình Python với thư viện NLTK để tìm số lượng tên nam và nữ trong các tên\n",
        "kho ngữ liệu. In tên 10 nam và nữ đầu tiên. Lưu ý: Kho văn bản tên chứa tổng cộng khoảng\n",
        "2943 nam (male.txt) và 5001 nữ (Female.txt) tên. Kho được biên soạn bởi Kantrowitz, Ross."
      ],
      "metadata": {
        "id": "Nv7_mQQMNq_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Tải tệp dữ liệu từ NLTK\n",
        "nltk.download('names')\n",
        "\n",
        "# Lấy danh sách các tên nam và nữ từ kho dữ liệu\n",
        "male_names = nltk.corpus.names.words('male.txt')\n",
        "female_names = nltk.corpus.names.words('female.txt')\n",
        "\n",
        "# In số lượng tên nam và nữ\n",
        "print(f\"Số lượng tên nam: {len(male_names)}\")\n",
        "print(f\"Số lượng tên nữ: {len(female_names)}\")\n",
        "print()\n",
        "\n",
        "# In 10 tên nam đầu tiên\n",
        "print(\"10 tên nam đầu tiên:\")\n",
        "print(male_names[:10])\n",
        "print()\n",
        "\n",
        "# In 10 tên nữ đầu tiên\n",
        "print(\"10 tên nữ đầu tiên:\")\n",
        "print(female_names[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw-HYt6XNy9Z",
        "outputId": "bc43b171-16db-485c-a0f9-bc7de0ad2e4e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng tên nam: 2943\n",
            "Số lượng tên nữ: 5001\n",
            "\n",
            "10 tên nam đầu tiên:\n",
            "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
            "\n",
            "10 tên nữ đầu tiên:\n",
            "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Viết chương trình Python với thư viện NLTK để in 15 kết hợp ngẫu nhiên đầu tiên được gắn\n",
        "nhãn nam và được gắn nhãn tên nữ từ kho tên."
      ],
      "metadata": {
        "id": "jPNTNg3lN0c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "\n",
        "# Tải tệp dữ liệu từ NLTK\n",
        "nltk.download('names')\n",
        "\n",
        "# Lấy danh sách các tên nam và nữ từ kho dữ liệu\n",
        "male_names = nltk.corpus.names.words('male.txt')\n",
        "female_names = nltk.corpus.names.words('female.txt')\n",
        "\n",
        "# Tạo danh sách 15 kết hợp ngẫu nhiên đầu tiên\n",
        "random.seed(1)  # Để đảm bảo kết quả giống nhau mỗi lần chạy\n",
        "random_male_names = random.sample(male_names, 15)\n",
        "random_female_names = random.sample(female_names, 15)\n",
        "\n",
        "# In 15 kết hợp ngẫu nhiên đầu tiên được gắn nhãn là nam\n",
        "print(\"15 kết hợp ngẫu nhiên đầu tiên được gắn nhãn là nam:\")\n",
        "for name in random_male_names:\n",
        "    print(f\"Nam: {name}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# In 15 kết hợp ngẫu nhiên đầu tiên được gắn nhãn là nữ\n",
        "print(\"15 kết hợp ngẫu nhiên đầu tiên được gắn nhãn là nữ:\")\n",
        "for name in random_female_names:\n",
        "    print(f\"Nữ: {name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2ywft2oN7Yo",
        "outputId": "fbde1734-74f3-465f-8bcd-69c097219239"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 kết hợp ngẫu nhiên đầu tiên được gắn nhãn là nam:\n",
            "Nam: Cyrillus\n",
            "Nam: Shelby\n",
            "Nam: Barty\n",
            "Nam: Guido\n",
            "Nam: Clay\n",
            "Nam: Quigman\n",
            "Nam: Nicolas\n",
            "Nam: Paolo\n",
            "Nam: Udell\n",
            "Nam: Linoel\n",
            "Nam: Fonzie\n",
            "Nam: Byram\n",
            "Nam: Pietro\n",
            "Nam: Andonis\n",
            "Nam: Mace\n",
            "\n",
            "15 kết hợp ngẫu nhiên đầu tiên được gắn nhãn là nữ:\n",
            "Nữ: Michaeline\n",
            "Nữ: Zaria\n",
            "Nữ: Addis\n",
            "Nữ: Mureil\n",
            "Nữ: Holly\n",
            "Nữ: Gaby\n",
            "Nữ: Veronique\n",
            "Nữ: Catherine\n",
            "Nữ: Kamilah\n",
            "Nữ: Angelica\n",
            "Nữ: Amabel\n",
            "Nữ: Ami\n",
            "Nữ: Shirlee\n",
            "Nữ: Ailey\n",
            "Nữ: Luanna\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Viết chương trình Python với thư viện NLTK để trích xuất ký tự cuối cùng của tất cả các tên\n",
        "được gắn nhãn và tạo mảng mới với chữ cái cuối cùng của mỗi tên và nhãn được liên kết."
      ],
      "metadata": {
        "id": "8iIbT3kLN9va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Lấy danh sách các tên nam và nữ từ kho dữ liệu\n",
        "male_names = nltk.corpus.names.words('male.txt')\n",
        "female_names = nltk.corpus.names.words('female.txt')\n",
        "\n",
        "# Tạo mảng mới với chữ cái cuối cùng của mỗi tên và nhãn\n",
        "last_chars = []\n",
        "\n",
        "# Tạo mảng với các tên nam và nhãn 'male'\n",
        "for name in male_names:\n",
        "    if len(name) > 0:\n",
        "        last_chars.append((name[-1], 'male'))\n",
        "\n",
        "# Tạo mảng với các tên nữ và nhãn 'female'\n",
        "for name in female_names:\n",
        "    if len(name) > 0:\n",
        "        last_chars.append((name[-1], 'female'))\n",
        "\n",
        "# In ra mảng mới với chữ cái cuối cùng và nhãn\n",
        "print(\"Mảng mới với chữ cái cuối cùng và nhãn:\")\n",
        "print(last_chars[:20])  # Chỉ in 20 phần tử đầu tiên để xem kết quả\n"
      ],
      "metadata": {
        "id": "TvkzB69vODD4",
        "outputId": "9561a4b4-d494-47a9-a49e-05542fba0711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mảng mới với chữ cái cuối cùng và nhãn:\n",
            "[('r', 'male'), ('n', 'male'), ('y', 'male'), ('e', 'male'), ('t', 'male'), ('t', 'male'), ('y', 'male'), ('l', 'male'), ('l', 'male'), ('m', 'male'), ('h', 'male'), ('e', 'male'), ('l', 'male'), ('d', 'male'), ('r', 'male'), ('m', 'male'), ('m', 'male'), ('e', 'male'), ('r', 'male'), ('m', 'male')]\n"
          ]
        }
      ]
    }
  ]
}